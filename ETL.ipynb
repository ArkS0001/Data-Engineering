{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCLGhZozLc/xpjF0bpFsbH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArkS0001/Data-Engineering/blob/main/ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfNWjyAgiFxX",
        "outputId": "4b31f20e-03c1-4dd4-931b-e2c48cb360d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ETL pipeline...\n",
            "Extracted: {'id': 3194, 'amount': 434.87, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 3852, 'amount': 428.65, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 1841, 'amount': 121.34, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 3049, 'amount': 191.12, 'status': 'SUCCESS'}\n",
            "Transformed: {'id': 3049, 'amount': 191.12, 'status': 'SUCCESS', 'timestamp': '2024-12-08 11:00:49'}\n",
            "Data loaded successfully.\n",
            "Extracted: {'id': 7836, 'amount': 209.93, 'status': 'FAILED'}\n",
            "Data filtered out (not SUCCESS).\n",
            "ETL pipeline completed.\n",
            "\n",
            "Stored Transactions:\n",
            "(3049, 191.12, 'SUCCESS', '2024-12-08 11:00:49')\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Create an in-memory SQLite database\n",
        "def initialize_database():\n",
        "    connection = sqlite3.connect(\":memory:\")\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE transactions (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            amount REAL,\n",
        "            status TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    connection.commit()\n",
        "    return connection\n",
        "\n",
        "# Simulated data extraction\n",
        "def extract_data():\n",
        "    return {\n",
        "        \"id\": random.randint(1000, 9999),\n",
        "        \"amount\": round(random.uniform(10, 500), 2),\n",
        "        \"status\": random.choice([\"SUCCESS\", \"FAILED\", \"PENDING\"])\n",
        "    }\n",
        "\n",
        "# Data transformation: Add a timestamp and filter successful transactions\n",
        "def transform_data(data):\n",
        "    data[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return data if data[\"status\"] == \"SUCCESS\" else None\n",
        "\n",
        "# Load data into the SQLite database\n",
        "def load_data(connection, data):\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT INTO transactions (id, amount, status, timestamp)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    \"\"\", (data[\"id\"], data[\"amount\"], data[\"status\"], data[\"timestamp\"]))\n",
        "    connection.commit()\n",
        "\n",
        "# Real-time ETL pipeline demonstration\n",
        "def etl_pipeline(connection, iterations=10, delay=1):\n",
        "    print(\"Starting ETL pipeline...\")\n",
        "    for _ in range(iterations):\n",
        "        # Step 1: Extract\n",
        "        raw_data = extract_data()\n",
        "        print(f\"Extracted: {raw_data}\")\n",
        "\n",
        "        # Step 2: Transform\n",
        "        transformed_data = transform_data(raw_data)\n",
        "        if transformed_data:\n",
        "            print(f\"Transformed: {transformed_data}\")\n",
        "\n",
        "            # Step 3: Load\n",
        "            load_data(connection, transformed_data)\n",
        "            print(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            print(\"Data filtered out (not SUCCESS).\")\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    print(\"ETL pipeline completed.\")\n",
        "\n",
        "# Query the database for demonstration\n",
        "def query_data(connection):\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT * FROM transactions\")\n",
        "    rows = cursor.fetchall()\n",
        "    print(\"\\nStored Transactions:\")\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    conn = initialize_database()\n",
        "    etl_pipeline(conn, iterations=5, delay=2)\n",
        "    query_data(conn)\n",
        "    conn.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# Create an in-memory SQLite database\n",
        "def initialize_database():\n",
        "    connection = sqlite3.connect(\":memory:\")\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE transactions (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            amount REAL,\n",
        "            status TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    connection.commit()\n",
        "    return connection\n",
        "\n",
        "# Simulated data extraction\n",
        "def extract_data():\n",
        "    return {\n",
        "        \"id\": random.randint(1000, 9999),\n",
        "        \"amount\": round(random.uniform(10, 500), 2),\n",
        "        \"status\": random.choice([\"SUCCESS\", \"FAILED\", \"PENDING\"])\n",
        "    }\n",
        "\n",
        "# Data transformation: Add a timestamp and filter successful transactions\n",
        "def transform_data(data):\n",
        "    data[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return data if data[\"status\"] == \"SUCCESS\" else None\n",
        "\n",
        "# Load data into the SQLite database\n",
        "def load_data(connection, data):\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT INTO transactions (id, amount, status, timestamp)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    \"\"\", (data[\"id\"], data[\"amount\"], data[\"status\"], data[\"timestamp\"]))\n",
        "    connection.commit()\n",
        "\n",
        "# Query data from the database\n",
        "def query_latest_data(connection):\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT amount, timestamp FROM transactions ORDER BY timestamp ASC LIMIT 10\")\n",
        "    return cursor.fetchall()\n",
        "\n",
        "# Visualization setup\n",
        "def update_visualization(frame, connection, x_data, y_data, ax):\n",
        "    data = query_latest_data(connection)\n",
        "    if data:\n",
        "        x_data.clear()\n",
        "        y_data.clear()\n",
        "        for amount, timestamp in data:\n",
        "            x_data.append(timestamp)\n",
        "            y_data.append(amount)\n",
        "        ax.clear()\n",
        "        ax.plot(x_data, y_data, marker=\"o\", color=\"blue\", label=\"Transaction Amount\")\n",
        "        ax.set_title(\"Real-Time Transaction Amounts\")\n",
        "        ax.set_xlabel(\"Timestamp\")\n",
        "        ax.set_ylabel(\"Amount\")\n",
        "        ax.legend()\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "# Real-time ETL pipeline demonstration\n",
        "def etl_pipeline_with_visualization(connection, iterations=10, delay=1):\n",
        "    print(\"Starting ETL pipeline with visualization...\")\n",
        "\n",
        "    x_data, y_data = [], []\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    ani = FuncAnimation(fig, update_visualization, fargs=(connection, x_data, y_data, ax), interval=delay * 1000, cache_frame_data=False)\n",
        "\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Step 1: Extract\n",
        "        raw_data = extract_data()\n",
        "        print(f\"Extracted: {raw_data}\")\n",
        "\n",
        "        # Step 2: Transform\n",
        "        transformed_data = transform_data(raw_data)\n",
        "        if transformed_data:\n",
        "            print(f\"Transformed: {transformed_data}\")\n",
        "\n",
        "            # Step 3: Load\n",
        "            load_data(connection, transformed_data)\n",
        "            print(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            print(\"Data filtered out (not SUCCESS).\")\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    print(\"ETL pipeline completed.\")\n",
        "    plt.show()\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    conn = initialize_database()\n",
        "    etl_pipeline_with_visualization(conn, iterations=10, delay=2)  # Run for 10 iterations with 2-second intervals\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "0vo0sL3w7o0d",
        "outputId": "f791a4ee-c7b0-4af1-b71b-ac91ffefc8b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ETL pipeline with visualization...\n",
            "Extracted: {'id': 6347, 'amount': 201.14, 'status': 'FAILED'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 7175, 'amount': 404.64, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 6569, 'amount': 163.34, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 1216, 'amount': 135.02, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 3259, 'amount': 450.22, 'status': 'SUCCESS'}\n",
            "Transformed: {'id': 3259, 'amount': 450.22, 'status': 'SUCCESS', 'timestamp': '2024-12-08 11:03:41'}\n",
            "Data loaded successfully.\n",
            "Extracted: {'id': 1347, 'amount': 240.53, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 7215, 'amount': 57.99, 'status': 'SUCCESS'}\n",
            "Transformed: {'id': 7215, 'amount': 57.99, 'status': 'SUCCESS', 'timestamp': '2024-12-08 11:03:45'}\n",
            "Data loaded successfully.\n",
            "Extracted: {'id': 4734, 'amount': 222.58, 'status': 'FAILED'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 5303, 'amount': 285.45, 'status': 'PENDING'}\n",
            "Data filtered out (not SUCCESS).\n",
            "Extracted: {'id': 7986, 'amount': 238.9, 'status': 'FAILED'}\n",
            "Data filtered out (not SUCCESS).\n",
            "ETL pipeline completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAGyCAYAAAArj289AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAe50lEQVR4nO3db2zdVf3A8U/b0VuItAzn2m0WJyigAhturBYkBFNpIhnugaEOsi0LiMgkQKOy8WcV0XUqkCVSXBggPsENCRDCliJUFqLULG5rAnEbwTG2ENptKu0surL2+3tgqL+6Dna7/qE7r1dyH/Rwzv2eSw6DN9/bewuyLMsCAAAgUYVjvQEAAICxJIoAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApOUdRS+99FLMnTs3pk6dGgUFBfH0009/6JqNGzfGF7/4xcjlcvGZz3wmHn300SFsFQAAYPjlHUXd3d0xY8aMaGpqOqr5b7zxRlx++eVx6aWXRltbW9x8881x7bXXxnPPPZf3ZgEAAIZbQZZl2ZAXFxTEU089FfPmzTvinFtvvTXWr18fr776av/YN7/5zXjnnXeiubl5qJcGAAAYFhNG+gKtra1RU1MzYKy2tjZuvvnmI645ePBgHDx4sP/nvr6++Pvf/x4f//jHo6CgYKS2CgAAfMRlWRYHDhyIqVOnRmHh8HxEwohHUXt7e5SXlw8YKy8vj66urvjXv/4VJ5544mFrGhsb46677hrprQEAAOPUnj174pOf/OSwPNeIR9FQLFu2LOrr6/t/7uzsjNNOOy327NkTpaWlY7gzAABgLHV1dUVlZWWcfPLJw/acIx5FFRUV0dHRMWCso6MjSktLB71LFBGRy+Uil8sdNl5aWiqKAACAYf21mhH/nqLq6upoaWkZMPb8889HdXX1SF8aAADgQ+UdRf/85z+jra0t2traIuI/H7nd1tYWu3fvjoj/vPVt4cKF/fOvv/762LlzZ/zgBz+I7du3xwMPPBCPP/543HLLLcPzCgAAAI5B3lH05z//Oc4///w4//zzIyKivr4+zj///Fi+fHlERLz99tv9gRQR8elPfzrWr18fzz//fMyYMSPuvffeeOihh6K2tnaYXgIAAMDQHdP3FI2Wrq6uKCsri87OTr9TBAAACRuJNhjx3ykCAAD4KBNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDShhRFTU1NMX369CgpKYmqqqrYtGnTB85ftWpVnHXWWXHiiSdGZWVl3HLLLfHvf/97SBsGAAAYTnlH0bp166K+vj4aGhpiy5YtMWPGjKitrY29e/cOOv+xxx6LpUuXRkNDQ2zbti0efvjhWLduXdx2223HvHkAAIBjlXcU3XffffGtb30rFi9eHJ///Odj9erVcdJJJ8Ujjzwy6PyXX345Lrroorjqqqti+vTpcdlll8X8+fM/9O4SAADAaMgrinp6emLz5s1RU1Pz3ycoLIyamppobW0ddM2FF14Ymzdv7o+gnTt3xoYNG+JrX/vaEa9z8ODB6OrqGvAAAAAYCRPymbx///7o7e2N8vLyAePl5eWxffv2QddcddVVsX///vjyl78cWZbFoUOH4vrrr//At881NjbGXXfdlc/WAAAAhmTEP31u48aNsWLFinjggQdiy5Yt8eSTT8b69evj7rvvPuKaZcuWRWdnZ/9jz549I71NAAAgUXndKZo0aVIUFRVFR0fHgPGOjo6oqKgYdM2dd94ZCxYsiGuvvTYiIs4999zo7u6O6667Lm6//fYoLDy8y3K5XORyuXy2BgAAMCR53SkqLi6OWbNmRUtLS/9YX19ftLS0RHV19aBr3n333cPCp6ioKCIisizLd78AAADDKq87RRER9fX1sWjRopg9e3bMmTMnVq1aFd3d3bF48eKIiFi4cGFMmzYtGhsbIyJi7ty5cd9998X5558fVVVV8frrr8edd94Zc+fO7Y8jAACAsZJ3FNXV1cW+ffti+fLl0d7eHjNnzozm5ub+D1/YvXv3gDtDd9xxRxQUFMQdd9wRb731VnziE5+IuXPnxk9+8pPhexUAAABDVJCNg/ewdXV1RVlZWXR2dkZpaelYbwcAABgjI9EGI/7pcwAAAB9loggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASNqQoqipqSmmT58eJSUlUVVVFZs2bfrA+e+8804sWbIkpkyZErlcLs4888zYsGHDkDYMAAAwnCbku2DdunVRX18fq1evjqqqqli1alXU1tbGjh07YvLkyYfN7+npia9+9asxefLkeOKJJ2LatGnx5ptvximnnDIc+wcAADgmBVmWZfksqKqqigsuuCDuv//+iIjo6+uLysrKuPHGG2Pp0qWHzV+9enX8/Oc/j+3bt8cJJ5wwpE12dXVFWVlZdHZ2Rmlp6ZCeAwAAGP9Gog3yevtcT09PbN68OWpqav77BIWFUVNTE62trYOueeaZZ6K6ujqWLFkS5eXlcc4558SKFSuit7f3iNc5ePBgdHV1DXgAAACMhLyiaP/+/dHb2xvl5eUDxsvLy6O9vX3QNTt37ownnngient7Y8OGDXHnnXfGvffeGz/+8Y+PeJ3GxsYoKyvrf1RWVuazTQAAgKM24p8+19fXF5MnT44HH3wwZs2aFXV1dXH77bfH6tWrj7hm2bJl0dnZ2f/Ys2fPSG8TAABIVF4ftDBp0qQoKiqKjo6OAeMdHR1RUVEx6JopU6bECSecEEVFRf1jn/vc56K9vT16enqiuLj4sDW5XC5yuVw+WwMAABiSvO4UFRcXx6xZs6KlpaV/rK+vL1paWqK6unrQNRdddFG8/vrr0dfX1z/22muvxZQpUwYNIgAAgNGU99vn6uvrY82aNfHrX/86tm3bFt/5zneiu7s7Fi9eHBERCxcujGXLlvXP/853vhN///vf46abborXXnst1q9fHytWrIglS5YM36sAAAAYory/p6iuri727dsXy5cvj/b29pg5c2Y0Nzf3f/jC7t27o7Dwv61VWVkZzz33XNxyyy1x3nnnxbRp0+Kmm26KW2+9dfheBQAAwBDl/T1FY8H3FAEAABEfge8pAgAAON6IIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaUOKoqamppg+fXqUlJREVVVVbNq06ajWrV27NgoKCmLevHlDuSwAAMCwyzuK1q1bF/X19dHQ0BBbtmyJGTNmRG1tbezdu/cD1+3atSu+973vxcUXXzzkzQIAAAy3vKPovvvui29961uxePHi+PznPx+rV6+Ok046KR555JEjrunt7Y2rr7467rrrrjj99NOPacMAAADDKa8o6unpic2bN0dNTc1/n6CwMGpqaqK1tfWI6370ox/F5MmT45prrjmq6xw8eDC6uroGPAAAAEZCXlG0f//+6O3tjfLy8gHj5eXl0d7ePuiaP/zhD/Hwww/HmjVrjvo6jY2NUVZW1v+orKzMZ5sAAABHbUQ/fe7AgQOxYMGCWLNmTUyaNOmo1y1btiw6Ozv7H3v27BnBXQIAACmbkM/kSZMmRVFRUXR0dAwY7+joiIqKisPm//Wvf41du3bF3Llz+8f6+vr+c+EJE2LHjh1xxhlnHLYul8tFLpfLZ2sAAABDktedouLi4pg1a1a0tLT0j/X19UVLS0tUV1cfNv/ss8+OV155Jdra2vofV1xxRVx66aXR1tbmbXEAAMCYy+tOUUREfX19LFq0KGbPnh1z5syJVatWRXd3dyxevDgiIhYuXBjTpk2LxsbGKCkpiXPOOWfA+lNOOSUi4rBxAACAsZB3FNXV1cW+ffti+fLl0d7eHjNnzozm5ub+D1/YvXt3FBaO6K8qAQAADJuCLMuysd7Eh+nq6oqysrLo7OyM0tLSsd4OAAAwRkaiDdzSAQAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkDSmKmpqaYvr06VFSUhJVVVWxadOmI85ds2ZNXHzxxTFx4sSYOHFi1NTUfOB8AACA0ZR3FK1bty7q6+ujoaEhtmzZEjNmzIja2trYu3fvoPM3btwY8+fPjxdffDFaW1ujsrIyLrvssnjrrbeOefMAAADHqiDLsiyfBVVVVXHBBRfE/fffHxERfX19UVlZGTfeeGMsXbr0Q9f39vbGxIkT4/7774+FCxce1TW7urqirKwsOjs7o7S0NJ/tAgAAx5GRaIO87hT19PTE5s2bo6am5r9PUFgYNTU10draelTP8e6778Z7770Xp5566hHnHDx4MLq6ugY8AAAARkJeUbR///7o7e2N8vLyAePl5eXR3t5+VM9x6623xtSpUweE1f9qbGyMsrKy/kdlZWU+2wQAADhqo/rpcytXroy1a9fGU089FSUlJUect2zZsujs7Ox/7NmzZxR3CQAApGRCPpMnTZoURUVF0dHRMWC8o6MjKioqPnDtPffcEytXrowXXnghzjvvvA+cm8vlIpfL5bM1AACAIcnrTlFxcXHMmjUrWlpa+sf6+vqipaUlqqurj7juZz/7Wdx9993R3Nwcs2fPHvpuAQAAhlled4oiIurr62PRokUxe/bsmDNnTqxatSq6u7tj8eLFERGxcOHCmDZtWjQ2NkZExE9/+tNYvnx5PPbYYzF9+vT+3z362Mc+Fh/72MeG8aUAAADkL+8oqquri3379sXy5cujvb09Zs6cGc3Nzf0fvrB79+4oLPzvDahf/vKX0dPTE9/4xjcGPE9DQ0P88Ic/PLbdAwAAHKO8v6doLPieIgAAIOIj8D1FAAAAxxtRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkbUhR1NTUFNOnT4+SkpKoqqqKTZs2feD83/72t3H22WdHSUlJnHvuubFhw4YhbRYAAGC45R1F69ati/r6+mhoaIgtW7bEjBkzora2Nvbu3Tvo/Jdffjnmz58f11xzTWzdujXmzZsX8+bNi1dfffWYNw8AAHCsCrIsy/JZUFVVFRdccEHcf//9ERHR19cXlZWVceONN8bSpUsPm19XVxfd3d3x7LPP9o996UtfipkzZ8bq1auP6ppdXV1RVlYWnZ2dUVpams92AQCA48hItMGEfCb39PTE5s2bY9myZf1jhYWFUVNTE62trYOuaW1tjfr6+gFjtbW18fTTTx/xOgcPHoyDBw/2/9zZ2RkR//kbAAAApOv9Jsjz3s4HyiuK9u/fH729vVFeXj5gvLy8PLZv3z7omvb29kHnt7e3H/E6jY2Ncddddx02XllZmc92AQCA49Tf/va3KCsrG5bnyiuKRsuyZcsG3F1655134lOf+lTs3r172F44DKarqysqKytjz5493qrJiHLWGC3OGqPFWWO0dHZ2xmmnnRannnrqsD1nXlE0adKkKCoqio6OjgHjHR0dUVFRMeiaioqKvOZHRORyucjlcoeNl5WV+YeMUVFaWuqsMSqcNUaLs8ZocdYYLYWFw/ftQnk9U3FxccyaNStaWlr6x/r6+qKlpSWqq6sHXVNdXT1gfkTE888/f8T5AAAAoynvt8/V19fHokWLYvbs2TFnzpxYtWpVdHd3x+LFiyMiYuHChTFt2rRobGyMiIibbropLrnkkrj33nvj8ssvj7Vr18af//znePDBB4f3lQAAAAxB3lFUV1cX+/bti+XLl0d7e3vMnDkzmpub+z9MYffu3QNuZV144YXx2GOPxR133BG33XZbfPazn42nn346zjnnnKO+Zi6Xi4aGhkHfUgfDyVljtDhrjBZnjdHirDFaRuKs5f09RQAAAMeT4fvtJAAAgHFIFAEAAEkTRQAAQNJEEQAAkLSPTBQ1NTXF9OnTo6SkJKqqqmLTpk0fOP+3v/1tnH322VFSUhLnnntubNiwYZR2yniXz1lbs2ZNXHzxxTFx4sSYOHFi1NTUfOjZhPfl++fa+9auXRsFBQUxb968kd0gx418z9o777wTS5YsiSlTpkQul4szzzzTv0c5KvmetVWrVsVZZ50VJ554YlRWVsYtt9wS//73v0dpt4xHL730UsydOzemTp0aBQUF8fTTT3/omo0bN8YXv/jFyOVy8ZnPfCYeffTRvK/7kYiidevWRX19fTQ0NMSWLVtixowZUVtbG3v37h10/ssvvxzz58+Pa665JrZu3Rrz5s2LefPmxauvvjrKO2e8yfesbdy4MebPnx8vvvhitLa2RmVlZVx22WXx1ltvjfLOGW/yPWvv27VrV3zve9+Liy++eJR2yniX71nr6emJr371q7Fr16544oknYseOHbFmzZqYNm3aKO+c8Sbfs/bYY4/F0qVLo6GhIbZt2xYPP/xwrFu3Lm677bZR3jnjSXd3d8yYMSOampqOav4bb7wRl19+eVx66aXR1tYWN998c1x77bXx3HPP5Xfh7CNgzpw52ZIlS/p/7u3tzaZOnZo1NjYOOv/KK6/MLr/88gFjVVVV2be//e0R3SfjX75n7X8dOnQoO/nkk7Nf//rXI7VFjhNDOWuHDh3KLrzwwuyhhx7KFi1alH39618fhZ0y3uV71n75y19mp59+etbT0zNaW+Q4ke9ZW7JkSfaVr3xlwFh9fX120UUXjeg+OX5ERPbUU0994Jwf/OAH2Re+8IUBY3V1dVltbW1e1xrzO0U9PT2xefPmqKmp6R8rLCyMmpqaaG1tHXRNa2vrgPkREbW1tUecDxFDO2v/691334333nsvTj311JHaJseBoZ61H/3oRzF58uS45pprRmObHAeGctaeeeaZqK6ujiVLlkR5eXmcc845sWLFiujt7R2tbTMODeWsXXjhhbF58+b+t9jt3LkzNmzYEF/72tdGZc+kYbi6YMJwbmoo9u/fH729vVFeXj5gvLy8PLZv3z7omvb29kHnt7e3j9g+Gf+Gctb+16233hpTp0497B8++P+Gctb+8Ic/xMMPPxxtbW2jsEOOF0M5azt37ozf//73cfXVV8eGDRvi9ddfjxtuuCHee++9aGhoGI1tMw4N5axdddVVsX///vjyl78cWZbFoUOH4vrrr/f2OYbVkbqgq6sr/vWvf8WJJ554VM8z5neKYLxYuXJlrF27Np566qkoKSkZ6+1wHDlw4EAsWLAg1qxZE5MmTRrr7XCc6+vri8mTJ8eDDz4Ys2bNirq6urj99ttj9erVY701jjMbN26MFStWxAMPPBBbtmyJJ598MtavXx933333WG8NDjPmd4omTZoURUVF0dHRMWC8o6MjKioqBl1TUVGR13yIGNpZe98999wTK1eujBdeeCHOO++8kdwmx4F8z9pf//rX2LVrV8ydO7d/rK+vLyIiJkyYEDt27IgzzjhjZDfNuDSUP9emTJkSJ5xwQhQVFfWPfe5zn4v29vbo6emJ4uLiEd0z49NQztqdd94ZCxYsiGuvvTYiIs4999zo7u6O6667Lm6//fYoLPT/5jl2R+qC0tLSo75LFPERuFNUXFwcs2bNipaWlv6xvr6+aGlpierq6kHXVFdXD5gfEfH8888fcT5EDO2sRUT87Gc/i7vvvjuam5tj9uzZo7FVxrl8z9rZZ58dr7zySrS1tfU/rrjiiv5P0qmsrBzN7TOODOXPtYsuuihef/31/vCOiHjttddiypQpgogjGspZe/fddw8Ln/dj/D+/Qw/Hbti6IL/PgBgZa9euzXK5XPboo49mf/nLX7LrrrsuO+WUU7L29vYsy7JswYIF2dKlS/vn//GPf8wmTJiQ3XPPPdm2bduyhoaG7IQTTsheeeWVsXoJjBP5nrWVK1dmxcXF2RNPPJG9/fbb/Y8DBw6M1UtgnMj3rP0vnz7H0cr3rO3evTs7+eSTs+9+97vZjh07smeffTabPHly9uMf/3isXgLjRL5nraGhITv55JOz3/zmN9nOnTuz3/3ud9kZZ5yRXXnllWP1EhgHDhw4kG3dujXbunVrFhHZfffdl23dujV78803syzLsqVLl2YLFizon79z587spJNOyr7//e9n27Zty5qamrKioqKsubk5r+t+JKIoy7LsF7/4RXbaaadlxcXF2Zw5c7I//elP/X/tkksuyRYtWjRg/uOPP56deeaZWXFxcfaFL3whW79+/SjvmPEqn7P2qU99KouIwx4NDQ2jv3HGnXz/XPv/RBH5yPesvfzyy1lVVVWWy+Wy008/PfvJT36SHTp0aJR3zXiUz1l77733sh/+8IfZGWeckZWUlGSVlZXZDTfckP3jH/8Y/Y0zbrz44ouD/rfX+2dr0aJF2SWXXHLYmpkzZ2bFxcXZ6aefnv3qV7/K+7oFWeb+JQAAkK4x/50iAACAsSSKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASNr/AUOP/hLIsQ49AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize SQLite database\n",
        "def initialize_database():\n",
        "    connection = sqlite3.connect(\":memory:\")\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE transactions (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            amount REAL,\n",
        "            status TEXT,\n",
        "            region TEXT,\n",
        "            category TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    connection.commit()\n",
        "    return connection\n",
        "\n",
        "# Simulate data extraction\n",
        "def extract_data(batch_size=5):\n",
        "    return [\n",
        "        {\n",
        "            \"id\": random.randint(1000, 9999),\n",
        "            \"amount\": round(random.uniform(10, 500), 2),\n",
        "            \"status\": random.choice([\"SUCCESS\", \"FAILED\", \"PENDING\"]),\n",
        "            \"region\": random.choice([\"North\", \"South\", \"East\", \"West\"])\n",
        "        }\n",
        "        for _ in range(batch_size)\n",
        "    ]\n",
        "\n",
        "# Data transformation\n",
        "def transform_data(batch):\n",
        "    transformed_batch = []\n",
        "    for record in batch:\n",
        "        # Data quality check: Filter invalid data\n",
        "        if record[\"amount\"] <= 0 or not record[\"status\"]:\n",
        "            print(f\"Invalid record discarded: {record}\")\n",
        "            continue\n",
        "\n",
        "        # Normalize amount (scale between 0 and 1)\n",
        "        record[\"normalized_amount\"] = round(record[\"amount\"] / 500, 2)\n",
        "\n",
        "        # Categorize transactions\n",
        "        if record[\"amount\"] < 100:\n",
        "            record[\"category\"] = \"Low\"\n",
        "        elif record[\"amount\"] < 300:\n",
        "            record[\"category\"] = \"Medium\"\n",
        "        else:\n",
        "            record[\"category\"] = \"High\"\n",
        "\n",
        "        # Add timestamp\n",
        "        record[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        transformed_batch.append(record)\n",
        "    return transformed_batch\n",
        "\n",
        "# Load data into SQLite database\n",
        "def load_data(connection, batch):\n",
        "    cursor = connection.cursor()\n",
        "    for record in batch:\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO transactions (id, amount, status, region, category, timestamp)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (record[\"id\"], record[\"amount\"], record[\"status\"], record[\"region\"], record[\"category\"], record[\"timestamp\"]))\n",
        "    connection.commit()\n",
        "\n",
        "# Query data from the database\n",
        "def query_data(connection):\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT * FROM transactions ORDER BY timestamp DESC LIMIT 10\")\n",
        "    return cursor.fetchall()\n",
        "\n",
        "# Real-time ETL pipeline with enhancements\n",
        "def enhanced_etl_pipeline(connection, iterations=10, batch_size=5, delay=2):\n",
        "    print(\"Starting enhanced ETL pipeline...\")\n",
        "\n",
        "    for i in range(iterations):\n",
        "        print(f\"\\nIteration {i + 1}:\")\n",
        "\n",
        "        # Step 1: Extract\n",
        "        raw_data = extract_data(batch_size)\n",
        "        print(f\"Extracted batch: {raw_data}\")\n",
        "\n",
        "        # Step 2: Transform\n",
        "        transformed_data = transform_data(raw_data)\n",
        "        print(f\"Transformed batch: {transformed_data}\")\n",
        "\n",
        "        # Step 3: Load\n",
        "        if transformed_data:\n",
        "            load_data(connection, transformed_data)\n",
        "            print(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            print(\"No valid data to load.\")\n",
        "\n",
        "        # Step 4: Query for monitoring\n",
        "        latest_data = query_data(connection)\n",
        "        print(f\"Latest 10 records in database: {latest_data}\")\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    print(\"ETL pipeline completed.\")\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    conn = initialize_database()\n",
        "    enhanced_etl_pipeline(conn, iterations=5, batch_size=3, delay=2)\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0RS6fcU7oqB",
        "outputId": "ef9a69b2-b786-4df5-beaa-cb4f6d3b4591"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting enhanced ETL pipeline...\n",
            "\n",
            "Iteration 1:\n",
            "Extracted batch: [{'id': 2122, 'amount': 172.44, 'status': 'FAILED', 'region': 'West'}, {'id': 4289, 'amount': 167.26, 'status': 'PENDING', 'region': 'South'}, {'id': 4039, 'amount': 163.09, 'status': 'PENDING', 'region': 'North'}]\n",
            "Transformed batch: [{'id': 2122, 'amount': 172.44, 'status': 'FAILED', 'region': 'West', 'normalized_amount': 0.34, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:23'}, {'id': 4289, 'amount': 167.26, 'status': 'PENDING', 'region': 'South', 'normalized_amount': 0.33, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:23'}, {'id': 4039, 'amount': 163.09, 'status': 'PENDING', 'region': 'North', 'normalized_amount': 0.33, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:23'}]\n",
            "Data loaded successfully.\n",
            "Latest 10 records in database: [(2122, 172.44, 'FAILED', 'West', 'Medium', '2024-12-08 11:06:23'), (4039, 163.09, 'PENDING', 'North', 'Medium', '2024-12-08 11:06:23'), (4289, 167.26, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:23')]\n",
            "\n",
            "Iteration 2:\n",
            "Extracted batch: [{'id': 8952, 'amount': 475.39, 'status': 'SUCCESS', 'region': 'North'}, {'id': 4467, 'amount': 499.68, 'status': 'FAILED', 'region': 'West'}, {'id': 9879, 'amount': 12.2, 'status': 'PENDING', 'region': 'North'}]\n",
            "Transformed batch: [{'id': 8952, 'amount': 475.39, 'status': 'SUCCESS', 'region': 'North', 'normalized_amount': 0.95, 'category': 'High', 'timestamp': '2024-12-08 11:06:25'}, {'id': 4467, 'amount': 499.68, 'status': 'FAILED', 'region': 'West', 'normalized_amount': 1.0, 'category': 'High', 'timestamp': '2024-12-08 11:06:25'}, {'id': 9879, 'amount': 12.2, 'status': 'PENDING', 'region': 'North', 'normalized_amount': 0.02, 'category': 'Low', 'timestamp': '2024-12-08 11:06:25'}]\n",
            "Data loaded successfully.\n",
            "Latest 10 records in database: [(4467, 499.68, 'FAILED', 'West', 'High', '2024-12-08 11:06:25'), (8952, 475.39, 'SUCCESS', 'North', 'High', '2024-12-08 11:06:25'), (9879, 12.2, 'PENDING', 'North', 'Low', '2024-12-08 11:06:25'), (2122, 172.44, 'FAILED', 'West', 'Medium', '2024-12-08 11:06:23'), (4039, 163.09, 'PENDING', 'North', 'Medium', '2024-12-08 11:06:23'), (4289, 167.26, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:23')]\n",
            "\n",
            "Iteration 3:\n",
            "Extracted batch: [{'id': 8747, 'amount': 237.14, 'status': 'PENDING', 'region': 'South'}, {'id': 9301, 'amount': 92.6, 'status': 'FAILED', 'region': 'South'}, {'id': 9393, 'amount': 397.98, 'status': 'PENDING', 'region': 'North'}]\n",
            "Transformed batch: [{'id': 8747, 'amount': 237.14, 'status': 'PENDING', 'region': 'South', 'normalized_amount': 0.47, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:27'}, {'id': 9301, 'amount': 92.6, 'status': 'FAILED', 'region': 'South', 'normalized_amount': 0.19, 'category': 'Low', 'timestamp': '2024-12-08 11:06:27'}, {'id': 9393, 'amount': 397.98, 'status': 'PENDING', 'region': 'North', 'normalized_amount': 0.8, 'category': 'High', 'timestamp': '2024-12-08 11:06:27'}]\n",
            "Data loaded successfully.\n",
            "Latest 10 records in database: [(8747, 237.14, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:27'), (9301, 92.6, 'FAILED', 'South', 'Low', '2024-12-08 11:06:27'), (9393, 397.98, 'PENDING', 'North', 'High', '2024-12-08 11:06:27'), (4467, 499.68, 'FAILED', 'West', 'High', '2024-12-08 11:06:25'), (8952, 475.39, 'SUCCESS', 'North', 'High', '2024-12-08 11:06:25'), (9879, 12.2, 'PENDING', 'North', 'Low', '2024-12-08 11:06:25'), (2122, 172.44, 'FAILED', 'West', 'Medium', '2024-12-08 11:06:23'), (4039, 163.09, 'PENDING', 'North', 'Medium', '2024-12-08 11:06:23'), (4289, 167.26, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:23')]\n",
            "\n",
            "Iteration 4:\n",
            "Extracted batch: [{'id': 1578, 'amount': 201.31, 'status': 'PENDING', 'region': 'West'}, {'id': 4758, 'amount': 42.27, 'status': 'SUCCESS', 'region': 'West'}, {'id': 3680, 'amount': 320.79, 'status': 'FAILED', 'region': 'West'}]\n",
            "Transformed batch: [{'id': 1578, 'amount': 201.31, 'status': 'PENDING', 'region': 'West', 'normalized_amount': 0.4, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:29'}, {'id': 4758, 'amount': 42.27, 'status': 'SUCCESS', 'region': 'West', 'normalized_amount': 0.08, 'category': 'Low', 'timestamp': '2024-12-08 11:06:29'}, {'id': 3680, 'amount': 320.79, 'status': 'FAILED', 'region': 'West', 'normalized_amount': 0.64, 'category': 'High', 'timestamp': '2024-12-08 11:06:29'}]\n",
            "Data loaded successfully.\n",
            "Latest 10 records in database: [(1578, 201.31, 'PENDING', 'West', 'Medium', '2024-12-08 11:06:29'), (3680, 320.79, 'FAILED', 'West', 'High', '2024-12-08 11:06:29'), (4758, 42.27, 'SUCCESS', 'West', 'Low', '2024-12-08 11:06:29'), (8747, 237.14, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:27'), (9301, 92.6, 'FAILED', 'South', 'Low', '2024-12-08 11:06:27'), (9393, 397.98, 'PENDING', 'North', 'High', '2024-12-08 11:06:27'), (4467, 499.68, 'FAILED', 'West', 'High', '2024-12-08 11:06:25'), (8952, 475.39, 'SUCCESS', 'North', 'High', '2024-12-08 11:06:25'), (9879, 12.2, 'PENDING', 'North', 'Low', '2024-12-08 11:06:25'), (2122, 172.44, 'FAILED', 'West', 'Medium', '2024-12-08 11:06:23')]\n",
            "\n",
            "Iteration 5:\n",
            "Extracted batch: [{'id': 4538, 'amount': 149.24, 'status': 'PENDING', 'region': 'East'}, {'id': 8690, 'amount': 469.86, 'status': 'FAILED', 'region': 'East'}, {'id': 6719, 'amount': 476.55, 'status': 'FAILED', 'region': 'East'}]\n",
            "Transformed batch: [{'id': 4538, 'amount': 149.24, 'status': 'PENDING', 'region': 'East', 'normalized_amount': 0.3, 'category': 'Medium', 'timestamp': '2024-12-08 11:06:31'}, {'id': 8690, 'amount': 469.86, 'status': 'FAILED', 'region': 'East', 'normalized_amount': 0.94, 'category': 'High', 'timestamp': '2024-12-08 11:06:31'}, {'id': 6719, 'amount': 476.55, 'status': 'FAILED', 'region': 'East', 'normalized_amount': 0.95, 'category': 'High', 'timestamp': '2024-12-08 11:06:31'}]\n",
            "Data loaded successfully.\n",
            "Latest 10 records in database: [(4538, 149.24, 'PENDING', 'East', 'Medium', '2024-12-08 11:06:31'), (6719, 476.55, 'FAILED', 'East', 'High', '2024-12-08 11:06:31'), (8690, 469.86, 'FAILED', 'East', 'High', '2024-12-08 11:06:31'), (1578, 201.31, 'PENDING', 'West', 'Medium', '2024-12-08 11:06:29'), (3680, 320.79, 'FAILED', 'West', 'High', '2024-12-08 11:06:29'), (4758, 42.27, 'SUCCESS', 'West', 'Low', '2024-12-08 11:06:29'), (8747, 237.14, 'PENDING', 'South', 'Medium', '2024-12-08 11:06:27'), (9301, 92.6, 'FAILED', 'South', 'Low', '2024-12-08 11:06:27'), (9393, 397.98, 'PENDING', 'North', 'High', '2024-12-08 11:06:27'), (4467, 499.68, 'FAILED', 'West', 'High', '2024-12-08 11:06:25')]\n",
            "ETL pipeline completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure your enhanced ETL pipeline is working correctly, you can use the following strategies to validate its behavior:\n",
        "1. Unit Testing Each Stage\n",
        "\n",
        "Test individual stages (Extract, Transform, Load) with predefined input and expected output.\n",
        "Example:\n",
        "\n",
        "    Extraction Stage: Check if the function generates the correct number of records with expected fields.\n",
        "\n",
        "data = extract_data(batch_size=3)\n",
        "assert len(data) == 3\n",
        "assert \"id\" in data[0] and \"amount\" in data[0]\n",
        "\n",
        "Transformation Stage: Validate data transformations.\n",
        "\n",
        "sample_data = [{\"id\": 1, \"amount\": 200, \"status\": \"SUCCESS\", \"region\": \"North\"}]\n",
        "transformed = transform_data(sample_data)\n",
        "assert transformed[0][\"category\"] == \"Medium\"\n",
        "\n",
        "Loading Stage: Verify data is inserted into the database correctly.\n",
        "\n",
        "    load_data(connection, transformed)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM transactions\")\n",
        "    assert cursor.fetchone()[0] > 0\n",
        "\n",
        "2. Data Quality Checks\n",
        "\n",
        "Ensure that:\n",
        "\n",
        "    Invalid or incomplete records are filtered out.\n",
        "    Transformed data aligns with your transformation logic.\n",
        "\n",
        "Example:\n",
        "\n",
        "    Count the number of records with status = 'SUCCESS':\n",
        "\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM transactions WHERE status = 'SUCCESS'\")\n",
        "    print(\"SUCCESS transactions:\", cursor.fetchone()[0])\n",
        "\n",
        "3. Monitoring and Logging\n",
        "\n",
        "Add extensive logs to track each stage of the pipeline. Logs help you identify where issues occur.\n",
        "Example:\n",
        "\n",
        "Add print statements or logging:\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "logging.info(\"Starting transformation...\")\n",
        "logging.info(f\"Transformed batch: {transformed_data}\")\n",
        "\n",
        "4. Query the Database\n",
        "\n",
        "Check the database for:\n",
        "\n",
        "    Correct number of records.\n",
        "    Data integrity (e.g., amount > 0 and status is valid).\n",
        "\n",
        "Example Queries:\n",
        "\n",
        "    Check total records:\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM transactions\")\n",
        "print(\"Total records:\", cursor.fetchone()[0])\n",
        "\n",
        "Check recent records:\n",
        "\n",
        "    cursor.execute(\"SELECT * FROM transactions ORDER BY timestamp DESC LIMIT 5\")\n",
        "    print(cursor.fetchall())\n",
        "\n",
        "5. Simulate Errors\n",
        "\n",
        "Introduce errors in each stage to test the pipeline's resilience.\n",
        "\n",
        "    Extraction: Simulate missing fields in extracted data.\n",
        "    Transformation: Pass invalid records and verify they are excluded.\n",
        "    Loading: Simulate database connection issues and ensure proper error handling.\n",
        "\n",
        "6. Real-Time Metrics\n",
        "\n",
        "For continuous monitoring, integrate metrics and visualize using tools like:\n",
        "\n",
        "    Grafana (via a Prometheus exporter).\n",
        "    Custom Dashboards to display the pipelineâ€™s progress and performance.\n",
        "\n",
        "7. End-to-End Validation\n",
        "\n",
        "Simulate a complete run and compare the results with expected outputs.\n",
        "\n",
        "    Extract a fixed dataset (mock extraction).\n",
        "    Apply transformations and log transformed results.\n",
        "    Load data and query the database to verify correctness.\n",
        "\n",
        "Example Validation Script\n",
        "\n",
        "# Validate the pipeline\n",
        "raw_data = extract_data(batch_size=3)\n",
        "print(\"Extracted Data:\", raw_data)\n",
        "\n",
        "transformed = transform_data(raw_data)\n",
        "print(\"Transformed Data:\", transformed)\n",
        "\n",
        "load_data(connection, transformed)\n",
        "print(\"Data loaded. Querying database...\")\n",
        "\n",
        "cursor = connection.cursor()\n",
        "cursor.execute(\"SELECT * FROM transactions\")\n",
        "print(\"Database Records:\", cursor.fetchall())\n",
        "\n",
        "8. Use Test Data\n",
        "\n",
        "Run the pipeline with a predefined dataset to compare the output with expected results.\n",
        "Test Input:\n",
        "\n",
        "test_data = [\n",
        "    {\"id\": 1, \"amount\": 250, \"status\": \"SUCCESS\", \"region\": \"North\"},\n",
        "    {\"id\": 2, \"amount\": -10, \"status\": \"PENDING\", \"region\": \"South\"},\n",
        "    {\"id\": 3, \"amount\": 0, \"status\": \"SUCCESS\", \"region\": \"East\"}\n",
        "]\n",
        "\n",
        "Expected Output:\n",
        "\n",
        "    One record in the database after filtering and transformation.\n",
        "\n",
        "By following these methods, you can systematically verify that the pipeline is functioning as expected and debug any issues. Let me know if you want to add automated tests for these validations!"
      ],
      "metadata": {
        "id": "IRSpqzQ49UqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Initialize SQLite database\n",
        "def initialize_database():\n",
        "    connection = sqlite3.connect(\":memory:\")\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE transactions (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            amount REAL,\n",
        "            status TEXT,\n",
        "            region TEXT,\n",
        "            category TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    connection.commit()\n",
        "    return connection\n",
        "\n",
        "# Simulated data extraction from multiple sources\n",
        "def extract_from_api():\n",
        "    logging.info(\"Extracting data from API...\")\n",
        "    return {\n",
        "        \"id\": random.randint(1000, 9999),\n",
        "        \"amount\": round(random.uniform(10, 500), 2),\n",
        "        \"status\": random.choice([\"SUCCESS\", \"FAILED\", \"PENDING\"]),\n",
        "        \"region\": random.choice([\"North\", \"South\", \"East\", \"West\"])\n",
        "    }\n",
        "\n",
        "def extract_from_csv(file_path=\"data.csv\"):\n",
        "    logging.info(\"Extracting data from CSV...\")\n",
        "    with open(file_path, mode='r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        return list(reader)\n",
        "\n",
        "def extract_data(batch_size=5):\n",
        "    # Combine data from multiple sources\n",
        "    return [extract_from_api() for _ in range(batch_size)]\n",
        "\n",
        "# Data transformation\n",
        "def transform_data(batch):\n",
        "    logging.info(\"Transforming data...\")\n",
        "    transformed_batch = []\n",
        "    for record in batch:\n",
        "        # Validate record\n",
        "        if record[\"amount\"] <= 0 or not record[\"status\"]:\n",
        "            logging.warning(f\"Invalid record discarded: {record}\")\n",
        "            continue\n",
        "\n",
        "        # Normalize amount (scale between 0 and 1)\n",
        "        record[\"normalized_amount\"] = round(record[\"amount\"] / 500, 2)\n",
        "\n",
        "        # Categorize transactions\n",
        "        if record[\"amount\"] < 100:\n",
        "            record[\"category\"] = \"Low\"\n",
        "        elif record[\"amount\"] < 300:\n",
        "            record[\"category\"] = \"Medium\"\n",
        "        else:\n",
        "            record[\"category\"] = \"High\"\n",
        "\n",
        "        # Add timestamp\n",
        "        record[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        transformed_batch.append(record)\n",
        "    return transformed_batch\n",
        "\n",
        "# Load data into SQLite database\n",
        "def load_data(connection, batch):\n",
        "    logging.info(\"Loading data into database...\")\n",
        "    cursor = connection.cursor()\n",
        "    for record in batch:\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO transactions (id, amount, status, region, category, timestamp)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (record[\"id\"], record[\"amount\"], record[\"status\"], record[\"region\"], record[\"category\"], record[\"timestamp\"]))\n",
        "    connection.commit()\n",
        "\n",
        "# Backup raw and processed data\n",
        "def backup_data(data, file_path=\"backup.csv\"):\n",
        "    logging.info(\"Backing up data...\")\n",
        "    with open(file_path, mode='a', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Notification service\n",
        "def notify_completion(message):\n",
        "    logging.info(f\"Notification: {message}\")\n",
        "\n",
        "# ETL pipeline with realistic services\n",
        "def etl_pipeline_with_services(connection, iterations=5, batch_size=3, delay=2):\n",
        "    logging.info(\"Starting ETL pipeline...\")\n",
        "    for i in range(iterations):\n",
        "        logging.info(f\"Iteration {i + 1}\")\n",
        "\n",
        "        # Step 1: Extract\n",
        "        raw_data = extract_data(batch_size)\n",
        "        logging.info(f\"Extracted batch: {raw_data}\")\n",
        "\n",
        "        # Backup raw data\n",
        "        backup_data(raw_data, \"raw_data_backup.csv\")\n",
        "\n",
        "        # Step 2: Transform\n",
        "        transformed_data = transform_data(raw_data)\n",
        "        logging.info(f\"Transformed batch: {transformed_data}\")\n",
        "\n",
        "        # Step 3: Load\n",
        "        if transformed_data:\n",
        "            load_data(connection, transformed_data)\n",
        "            logging.info(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            logging.warning(\"No valid data to load.\")\n",
        "\n",
        "        # Backup transformed data\n",
        "        backup_data(transformed_data, \"processed_data_backup.csv\")\n",
        "\n",
        "        # Step 4: Query for monitoring\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(\"SELECT * FROM transactions ORDER BY timestamp DESC LIMIT 5\")\n",
        "        latest_data = cursor.fetchall()\n",
        "        logging.info(f\"Latest 5 records: {latest_data}\")\n",
        "\n",
        "        # Notify completion of iteration\n",
        "        notify_completion(f\"Iteration {i + 1} completed successfully.\")\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    logging.info(\"ETL pipeline completed.\")\n",
        "\n",
        "# Run the demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    conn = initialize_database()\n",
        "\n",
        "    # Run the ETL pipeline in a separate thread\n",
        "    pipeline_thread = threading.Thread(target=etl_pipeline_with_services, args=(conn,))\n",
        "    pipeline_thread.start()\n",
        "    pipeline_thread.join()\n",
        "\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70rs6tXI96Ak",
        "outputId": "12122ea2-36c4-4dd6-a2d6-987c2fb762ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-11 (etl_pipeline_with_services):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-8-8dacdbc786d3>\", line 118, in etl_pipeline_with_services\n",
            "  File \"<ipython-input-8-8dacdbc786d3>\", line 79, in load_data\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 136069931003904 and this is thread id 136069152482880.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Initialize SQLite database\n",
        "def initialize_database():\n",
        "    connection = sqlite3.connect(\"etl_pipeline.db\")  # Use a file-based DB for persistence\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS transactions (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            amount REAL,\n",
        "            status TEXT,\n",
        "            region TEXT,\n",
        "            category TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    connection.commit()\n",
        "    connection.close()  # Close immediately to prevent threading issues\n",
        "\n",
        "# Simulated data extraction from multiple sources\n",
        "def extract_from_api():\n",
        "    return {\n",
        "        \"id\": random.randint(1000, 9999),\n",
        "        \"amount\": round(random.uniform(10, 500), 2),\n",
        "        \"status\": random.choice([\"SUCCESS\", \"FAILED\", \"PENDING\"]),\n",
        "        \"region\": random.choice([\"North\", \"South\", \"East\", \"West\"])\n",
        "    }\n",
        "\n",
        "def extract_from_csv(file_path=\"data.csv\"):\n",
        "    with open(file_path, mode='r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        return list(reader)\n",
        "\n",
        "def extract_data(batch_size=5):\n",
        "    return [extract_from_api() for _ in range(batch_size)]\n",
        "\n",
        "# Data transformation\n",
        "def transform_data(batch):\n",
        "    transformed_batch = []\n",
        "    for record in batch:\n",
        "        if record[\"amount\"] <= 0 or not record[\"status\"]:\n",
        "            continue\n",
        "        record[\"normalized_amount\"] = round(record[\"amount\"] / 500, 2)\n",
        "        record[\"category\"] = (\n",
        "            \"Low\" if record[\"amount\"] < 100 else\n",
        "            \"Medium\" if record[\"amount\"] < 300 else\n",
        "            \"High\"\n",
        "        )\n",
        "        record[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        transformed_batch.append(record)\n",
        "    return transformed_batch\n",
        "\n",
        "# Load data into SQLite database\n",
        "def load_data(batch):\n",
        "    connection = sqlite3.connect(\"etl_pipeline.db\")\n",
        "    cursor = connection.cursor()\n",
        "    for record in batch:\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO transactions (id, amount, status, region, category, timestamp)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (record[\"id\"], record[\"amount\"], record[\"status\"], record[\"region\"],\n",
        "              record[\"category\"], record[\"timestamp\"]))\n",
        "    connection.commit()\n",
        "    connection.close()\n",
        "\n",
        "# Backup raw and processed data\n",
        "def backup_data(data, file_path):\n",
        "    with open(file_path, mode='a', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# ETL pipeline with proper threading safety\n",
        "def etl_pipeline_with_services(iterations=5, batch_size=3, delay=2):\n",
        "    logging.info(\"Starting ETL pipeline...\")\n",
        "    for i in range(iterations):\n",
        "        logging.info(f\"Iteration {i + 1}\")\n",
        "\n",
        "        # Step 1: Extract\n",
        "        raw_data = extract_data(batch_size)\n",
        "        logging.info(f\"Extracted batch: {raw_data}\")\n",
        "        backup_data(raw_data, \"raw_data_backup.csv\")\n",
        "\n",
        "        # Step 2: Transform\n",
        "        transformed_data = transform_data(raw_data)\n",
        "        logging.info(f\"Transformed batch: {transformed_data}\")\n",
        "        backup_data(transformed_data, \"processed_data_backup.csv\")\n",
        "\n",
        "        # Step 3: Load\n",
        "        if transformed_data:\n",
        "            load_data(transformed_data)\n",
        "            logging.info(\"Data loaded successfully.\")\n",
        "        else:\n",
        "            logging.warning(\"No valid data to load.\")\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    logging.info(\"ETL pipeline completed.\")\n",
        "\n",
        "# Generate sample CSV file for testing\n",
        "def generate_sample_csv(file_path=\"data.csv\"):\n",
        "    data = [\n",
        "        {\"id\": 1, \"amount\": 250, \"status\": \"SUCCESS\", \"region\": \"North\"},\n",
        "        {\"id\": 2, \"amount\": -10, \"status\": \"PENDING\", \"region\": \"South\"},\n",
        "        {\"id\": 3, \"amount\": 0, \"status\": \"SUCCESS\", \"region\": \"East\"},\n",
        "        {\"id\": 4, \"amount\": 450, \"status\": \"FAILED\", \"region\": \"West\"},\n",
        "        {\"id\": 5, \"amount\": 150, \"status\": \"SUCCESS\", \"region\": \"North\"}\n",
        "    ]\n",
        "    with open(file_path, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"id\", \"amount\", \"status\", \"region\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    generate_sample_csv()  # Generate the CSV file\n",
        "    initialize_database()  # Initialize the database\n",
        "    etl_pipeline_with_services()  # Run the ETL pipeline\n"
      ],
      "metadata": {
        "id": "nllDtb3u-aCu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def print_csv_content(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Contents of {file_path}:\\n\")\n",
        "        print(df)\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{file_path} not found. Please ensure the ETL pipeline has run and created this file.\\n\")\n",
        "\n",
        "raw_data_path = \"/content/raw_data_backup.csv\"\n",
        "processed_data_path = \"/content/processed_data_backup.csv\"\n",
        "data_csv_path = \"/content/data.csv\"\n",
        "\n",
        "# Print the content of the CSV files\n",
        "print_csv_content(data_csv_path)\n",
        "print_csv_content(raw_data_path)\n",
        "print_csv_content(processed_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EfEMp2y-tFw",
        "outputId": "8b506514-1c15-409c-fb74-1f37c7bce9bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/data.csv:\n",
            "\n",
            "   id  amount   status region\n",
            "0   1     250  SUCCESS  North\n",
            "1   2     -10  PENDING  South\n",
            "2   3       0  SUCCESS   East\n",
            "3   4     450   FAILED   West\n",
            "4   5     150  SUCCESS  North\n",
            "\n",
            "==================================================\n",
            "\n",
            "Contents of /content/raw_data_backup.csv:\n",
            "\n",
            "      id  amount   status  region\n",
            "0   5310   92.04  SUCCESS   North\n",
            "1   2148   60.58   FAILED    West\n",
            "2   7962   94.42  SUCCESS    East\n",
            "3     id  amount   status  region\n",
            "4   6613  214.32   FAILED   South\n",
            "5   8946  432.34  PENDING   South\n",
            "6   2992  358.54   FAILED    East\n",
            "7     id  amount   status  region\n",
            "8   4772  238.93  SUCCESS   North\n",
            "9   8699  129.58  PENDING    West\n",
            "10  4333  400.36  PENDING   North\n",
            "11    id  amount   status  region\n",
            "12  2629   23.72  SUCCESS   North\n",
            "13  6255  482.68   FAILED   North\n",
            "14  6908  220.17   FAILED   South\n",
            "15    id  amount   status  region\n",
            "16  1109  219.27  PENDING    East\n",
            "17  8360   95.62   FAILED   South\n",
            "18  5441  115.85  SUCCESS   South\n",
            "19    id  amount   status  region\n",
            "20  5405  204.42  SUCCESS   North\n",
            "21  2856  399.13  SUCCESS   South\n",
            "22  5554  374.34  PENDING    West\n",
            "\n",
            "==================================================\n",
            "\n",
            "Contents of /content/processed_data_backup.csv:\n",
            "\n",
            "      id  amount   status  region  normalized_amount  category  \\\n",
            "0   6613  214.32   FAILED   South               0.43    Medium   \n",
            "1   8946  432.34  PENDING   South               0.86      High   \n",
            "2   2992  358.54   FAILED    East               0.72      High   \n",
            "3     id  amount   status  region  normalized_amount  category   \n",
            "4   4772  238.93  SUCCESS   North               0.48    Medium   \n",
            "5   8699  129.58  PENDING    West               0.26    Medium   \n",
            "6   4333  400.36  PENDING   North                0.8      High   \n",
            "7     id  amount   status  region  normalized_amount  category   \n",
            "8   2629   23.72  SUCCESS   North               0.05       Low   \n",
            "9   6255  482.68   FAILED   North               0.97      High   \n",
            "10  6908  220.17   FAILED   South               0.44    Medium   \n",
            "11    id  amount   status  region  normalized_amount  category   \n",
            "12  1109  219.27  PENDING    East               0.44    Medium   \n",
            "13  8360   95.62   FAILED   South               0.19       Low   \n",
            "14  5441  115.85  SUCCESS   South               0.23    Medium   \n",
            "15    id  amount   status  region  normalized_amount  category   \n",
            "16  5405  204.42  SUCCESS   North               0.41    Medium   \n",
            "17  2856  399.13  SUCCESS   South                0.8      High   \n",
            "18  5554  374.34  PENDING    West               0.75      High   \n",
            "\n",
            "              timestamp  \n",
            "0   2024-12-08 11:13:43  \n",
            "1   2024-12-08 11:13:43  \n",
            "2   2024-12-08 11:13:43  \n",
            "3             timestamp  \n",
            "4   2024-12-08 11:13:45  \n",
            "5   2024-12-08 11:13:45  \n",
            "6   2024-12-08 11:13:45  \n",
            "7             timestamp  \n",
            "8   2024-12-08 11:13:47  \n",
            "9   2024-12-08 11:13:47  \n",
            "10  2024-12-08 11:13:47  \n",
            "11            timestamp  \n",
            "12  2024-12-08 11:13:49  \n",
            "13  2024-12-08 11:13:49  \n",
            "14  2024-12-08 11:13:49  \n",
            "15            timestamp  \n",
            "16  2024-12-08 11:13:51  \n",
            "17  2024-12-08 11:13:51  \n",
            "18  2024-12-08 11:13:51  \n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}